\documentclass[11pt,a4paper,final,notitlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{url}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{varwidth}
\usepackage{hyperref}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{stackengine,graphicx,trimclip,scalerel}


\renewcommand{\baselinestretch}{1.25} %Increase 

\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother

\newcommand{\noNumberChapter}[2]{
    \setcounter{chapter}{#1}
    \setcounter{section}{0}
    \chapter*{#2}
    \addcontentsline{toc}{chapter}{#2}
}

\usepackage[nopar]{lipsum}
\usepackage{stackengine,graphicx,trimclip,scalerel}
\savestack\eye{\rotatebox{90}{$^\circ\mkern-6mu\raisebox{1pt}{)}$}}
\savestack\nose{\raisebox{3pt}{\scalebox{1}[-1]{\clipbox{0pt 1pt 0pt 0pt}{?}}}}
\savestack\mouth{\rotatebox{90}{(}}
\newcommand\Lenny{(\scalerel{\stackanchor[2pt]{\eye \nose \eye}{\mouth}}{)}}




\begin{document}

\title{ Major Project Report }

\author{ Tom Minor - Level H\\
 		 Major Project\\
 		 Bournemouth University, NCCA\\
 		 \\
 		 Supervised by Oleg Fryazinov, Adam Redford
		}

% Remove date
\date{}
\maketitle

\renewcommand{\abstractname}{Project Overview and Responsibilities}
%\\We have crazy integration, crazy asset production, crazy pipeline and our own GPU renderer - that's quite cool when you put it that way
\begin{abstract}

\begin{center}
\begin{varwidth}{\textwidth}
\begin{enumerate}
\item Fractal Renderer
\item FX
\item Pipeline\\
\end{enumerate}
\end{varwidth}
\end{center}

\textbf{CONTACT} is a near 3 minute long VFX sci-fi short, showing an astronautâ€™s state of mental decay after experiencing an encounter with a 5th dimensional being while in orbit. The team worked hard to create over 80 CG assets, 3 digital environments, and a bespoke fractal render engine for the evolving tunnel sequence at the height of the piece. Over 26 shots were composited into these built environments and costumes, fleshing out the narrative and blending together the live-action and the digital elements of the film.

\end{abstract}

\toccontents

\chapter{Introduction}
I originally planned to create a Fractal LookDev Tool by creating my own renderer and working alone on an R\&D project, as pathtracing and fractals are a combination that I really find fascinating. However, after talking to Kyran Bishop we both came up with an initial pitch strongly inspired by the film A Space Odyssey \cite{hal} that culminates in a dramatic psychedelic sequence. This gave me a fantastic opportunity to develop a fractal tool that would actually be put to the test by a team of artists and be integrated in a piece to create something visually stunning. 

As an R\&D technical person, it is always more satisfying and gratifying to create a tool with an actual purpose that will be used by other people. Joining a team of artists made my overall job harder as the result had to be absolutely polished and of professional quality which really pushed me to develop the tool the best I could. Within the group, I set out initially to work on the renderer alone but given the large amount of work we had to do as a group it was unrealistic to expect myself to work purely on that. This meant I took up additional roles such as Pipeline TD and wrote a plugin that encompasses Perforce in Maya as this only existed for Windows and not Linux, and FX TD to create a destruction effect for our piece.

\chapter{Initial Research}

\section{Fractal Sequence}
\subsection{Fractals in FX Software}

It would have made no sense to go ahead with the fractal tool idea if it was easily possible in existing renderers, Initially I looked into developing fractals in Houdini. The most common approach for the Mandelbulb  fractal shape I was focusing on was to calculate the voxel based shape and render it was a volume, The results were quite sucessful \ref{fig:voxelbulb} and while this did give an effect similiar to that achieved by Disney in the Big Hero 6 \cite{bh6} fractal sequence, it was far too slow to compute for any form of interactive tweaking to be used for our fractal sequence that would need several hundred frames rendered. 

After a little more research, I found out faster alternatives in Houdini created by the FX team responsible for the fractal sequence in \textit{Lucy}. They implemented a Vex based raymarcher \cite{Kim:2014:CIU:2614106.2614166} that worked and rendered a lot  faster than a volumetric approach, but it still suffered from interactivity issues that specialised solutions like Shadertoy do not. 

I decided Houdini wasn't the way to go for developing my tool but still used it several times during the project for several other tasks such as some creature FX R\&D work  \ref{fig:noise_shader} and Voyager destruction. Shadertoy and GPU Accelerated rendering of implicit surfaces seemed to be the efficient way of rendering, so I began moving in that direction for creating my custom lookdev tool.

\begin{figure}
\begin{center}
\includegraphics[scale=0.2]{"images/houdini_mandelbulb"}
\caption{Voxel Based Volumetric Mandelbulb}
\label{fig:voxelbulb}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.2]{"images/noise_shader"}
\caption{Houdini Plasma Shader Effect}
\label{fig:noise_shader}
\end{center}
\end{figure}


\subsection{Distance Estimated Fractals}


\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{images/raymarchstep}
\caption{Ray marching algorithm}
\label{fig:raymarching}
\end{center}
\end{figure}

The basic raymarching algorithm can be optimised through a technique known as Sphere Tracing \cite{Hart1996}, which requires knowing the nearest distance to a surface at any point. \ref{fig:sphere_tracing}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{images/spheretracing}
\caption{Sphere tracing algorithm}
\label{fig:sphere_tracing}
\end{center}
\end{figure}





\chapter{Dynamics FX}

Had to create the effect of debris slowly drifting through space, so created the genesis of the destruction sequence as a base of the effect.

\section{Setting up the simulation}
\begin{itemize}
	\item Import the Voyager model as an FBX (with UVs) and convert the Maya hierarchy setup to Houdini groups
	\item Fracture a slightly pre-deformed version of the mesh, transfer the UVs and create new ones in the gaps
	\item Generate constraints between all nearby fractured pieces
	\item Manually paint on the strengths of each fractured piece, this is done by painting the strength onto the vertices and then finding the average strength of each fractured piece's vertices
	\item The core unit of voyager has a very high strength so it does not fall apart, as it has to be intact for the shot
	\item The manually painted strength values are multiplied by noise to get more variation
	\item The strength values are remapped using a curve for a less linear falloff
	\item Finally, transfer the strength values to the constraints and feed into the simulation
\end{itemize}

\section{Simulation Elements}
\begin{itemize}
	\item Fractured voyager with constraints
	\item Phantom collision geometry that initiates the destruction effect
	\item Zero gravity simulation, with torque and spin POP nodes to introduce interesting rotations into the simulation.
\end{itemize}

\section{Final Touches}
In the end, the simulation was exported to an alembic cache and Lewis processed it with a simple script that baked the animation data into locators. Using this method, we preserved animation ability for individual pieces if necessary but otherwise the simulation looked identical. Slowing down the simulation 20x in the Alembic's time scale was sufficient to give a convincing effect that the debris was slowly drifting, without completely stopping the simulation.






\chapter{Pipeline}

\begin{itemize}
	\item Dropbox isn't enough
	\item Need to version
	\item More effort at beginning of the project, having backups of every asset could save our asses in the end
	\item use perforce
	\item Other scripts can be added to the main pipeline suite
	\item Referencing pipeline required custom hooks that modify all referenced paths to be relative to \$CONTACTROOT, automated, idiot proof
	\item Referencing in 10 files and manually clicking student popup is tedious, on save and submit remove student/education flags
	\item Forces the team to think in a collaborative way, they literally cannot work on the same asset at the same time
	\item Took some getting used to, but now the artists are used to version control and it's benefits even if it's tedious at times
	\item Main development time spent on PySide Qt GUI stuff, after the initial time spent learning the P4 API and commands
	\item Technically cross application, the P4 and GUI side of things will work whereever pyside is available. Needs a few app specific tweaks such as file saving commands etc to work properly but wouldn't take long to port
	\item Various wizards to automate asset/shot/lookdev file structure generation because kyran made the layout super complex
	\item Allows us to analyse trends
	\item I setup Zync on Linux
\end{itemize}


\includegraphics[width=\textwidth]{images/graph.png}





\chapter{Fractal Sequence}



\section{Fractal Tool Requirements}

\begin{itemize}
	\item How best to make this artist friendly (Nodegraph)
	\item What technique (ray marching)
	\item Use Optix because it's faster than what i can do
	\item Nodegraph needs runtime compilation, at least for geo
	\item Does require a little hacking to get runtime code generation
	\item Use NVRTC to compile code at runtime and plug into preexisting functions in the optix code
	\item Use hacky system commands to call nvcc directly if using CUDA 6.5 or less, aka, the uni systems
	\item Initial dev time for node graph, runtime compilation etc is long, but in theory will allow for rapid iteration once it works
	\item Everything is based on demo scene stuff, can use shadertoy as reference for loads of effects
	\section{Scene Management - Code Reflection}
		Need to change scene based on node graph
	\subsection{PTX Patching}
		\begin{itemize}
			\item Initial attempt, required research into the .ptx format
			\item Use NVCC to compile CUDA program into ptx code and patch into the Optix ptx code, then load into Optix
			\item Works on my machine and university workstations, but potentially undefined behaviour and not officially supported
			\item Relies on my own string handling functions
		\end{itemize}
	\subsection{Optix Callable Programs}
		\begin{itemize}
			\item Discovered this later in the project after reading the Optix documentation fully, initially didn't notice what it was because it's not used very often compared to the other aspects of optix and isn't really clear unless you know what it does.
			\item Use NVCC to compile CUDA program into ptx code and then tell Optix to use this as a 'callable program', basically replacing the ptx patching process with a well defined, built in functionality.
			
		\end{itemize}
\end{itemize}

\section{Rendering Strategy}
The world is sampled using a simple pinhole camera \cite{pinhole}

\begin{itemize}
	\item Render tiles \cite[pp. 51-52]{kalamp}
	\begin{itemize}
		\item For larger frame sizes this will give an opportunity for the calling program to return quickly and handle events
		\item Will use less GPU RAM, which is vital for HD frames on GPUs without lots of memory
		\item Has the potential to simplify implementation of other rendering algorithms in the future, for example Bidirectional Path Tracing stores light paths in an array that can become very large for huge images but is manageable for small tiles
	\end{itemize}

	\item Monitor the Optix rendering in a separate thread and copy over to host memory every half a second or so, this keeps the GUI responsive without over saturating the PCI-E bus (GPU -> CPU memory transfer) with constant memory copies.
	
\end{itemize}

\section{Output}
\begin{itemize}
	\item Save to EXR
	\item ZIP Compression, we need to render 650 1080p frames, a lot of data
	\item Multichannel
	\item Store each channel in a separate optix buffer
	\item Issue: Tedious to extend, but unlikely to be changed often
	
\end{itemize}

\subsection{Environment Camera}
\begin{itemize}
	\item Last minute request
	\item Required for relighting astronaut
	\item Optix makes this an easy thing to change, just fire rays in such a way that all possible directions are drawn at once like a fish eye lens.
	\item Thankfully the way I setup the various buffers maps directly onto the new camera model with no modification, adding the camera was as simple as defining a new ray generation program. This means that EXRs can be rendered with either the pinhole or environment camera and they will have identical data passes, this allowed us to use the exact same compositing networks to calculate the environment colour and have a 1:1 match for the relighting of the astronaut. Nuke and Maya accept the generated maps as environment maps with no issues.
\end{itemize}

\section{Colouring the fractal consistently}
\begin{itemize}
	\item Tried iteration count
	\item Tried comp only approaches
	\item Orbit traps are the way to go
	\item In the end we decided to use 3 different orbit traps encoded in the RGB channels of an EXR pass. This was configurable on a per scene basis if necessary (in the case of the tunnel scenes it was, the tube orbit trap gave useless values and needed manual tweaking)
\end{itemize}

\section{How the Shots were achieved}
\subsection{Side Sequence}

\begin{itemize}
	\item Use a mandelbulb, that slowly morphs from power 2 to power 5 as the sequence goes on. This
produces a growing effect.
	\item Apply a simple scene wide transformation that moves the mandelbulb slowly along the Z axis,
as well as rotates the fractal shape slowly around the Z axis.
	\item Set a low FOV of 30 to give the impression that the fractal shape is huge
\end{itemize}

\subsection{Tunnel Sequence}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{images/mengerjourney.png}
  \caption{Menger Journey \cite{mengerjourney}}
  \label{fig:menger}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{images/kaljourney.png}
  \caption{Kaleidoscopic Journey \cite{kaljourney}}
  \label{fig:kal}
\end{subfigure}
\caption{Two tunnel variations created by Syntopia, on Shadertoy}
\label{fig:test}
\end{figure}

The main tunnel shape was originally less organic, the base shape was heavily inspired by the work of Syntopia's Shadertoy demos \cite{mengerjourney} \cite{kaljourney}. 

My initial design was prototyped in shadertoy under the assumption I would port it over to the renderer, this prototype is visible here: \url{www.shadertoy.com/view/ls3SRs}. However, due to team feedback the design changed dramatically and only some elements survived into the iteration shown in \ref{fig:initialdesign}.

The main desire of the team was to maintain symmetry, which disallowed the use of the twist effect that created such interesting motion in the Shadertoy iteration. However, I took some more inspiration from \ref{fig:kal} and applied the base shape to my IFS fractal, this created a very interesting pulsing and rotating motion that everyone liked. We were planning to use this in the final cut, but then I dug deeper into the workings of \ref{fig:kal} and discovered how the interesting surface details are created using \textit{geometry traps}.

\ref{fig:finaldesign} shows the result of applying a Mandelbulb geometry trap to the shape of the original IFS tunnel, because it is simply stuck to the surface it follows the interesting rotating motion of the tunnel but has the organic surface of a bulb.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{images/ifs_tunnel.png}
  \caption{The initial design}
  \label{fig:initialdesign}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{images/tunnelbulb.png}
  \caption{The final design}
  \label{fig:finaldesign}
\end{subfigure}
\caption{Tunnel Iterations for the piece}
\label{fig:test}
\end{figure}

\subsection{Reflected Visor Sequence}
This one was done last

\begin{itemize}
\item Power 5 mandelbulb constrained within a manually tweaked box shape (via intersection function)
to cut off the majority of itâ€™s mass and keep itâ€™s shape light and tendril like
\item This entire shape is then intersected with the tunnel shape, combining the organic shape of
the Mandelbulb with the hole in the tunnel. This creates an interesting silhouette.
\item Primarily used as a silhouette in the comp, so not many samples were required for usable
frames.
\item A high FOV of 110 is used to stretch out the shape even more and ease the process of mapping
it onto the visor.
\end{itemize}

\chapter{Future Improvements}

\begin{itemize}
	\item Look into methods besides distance estimation for more fractal possibilities, brute force approach
	\item Make the interface more artist friendly, with less reliance on understanding the maths
	\item Improve the input handling and allow for multiple input methods such as gamepads
	\item Some form of adaptive rendering to keep the viewport responsive
	\item Improve the light sampling algorithms using methods such as Bidirectional Path Tracing, VCM or Metropolis light transport.
	\item Provide a GUI interface for the lighting setup
	\item Move the batch rendering script's capabilities directly into the main renderer
	\item Screenspace CUDA shaders that make use of the various buffers available	
\end{itemize}


\chapter{Conclusion}

\paragraph{In conclusion} I believe Romanesco is a useful and novel way of developing fractal objects in a visual effects context. The majority of currently available renderers are more of a novelty, designed to create nice images for a desktop background instead of a usable plate. Attempting to use existing renderers to create fractals is a plausible method, but suffers from poor interactivity due to long calculation times for the fractal structure.

Romanesco begins to fill in the gap between the currently available approaches, with more polish and focus put towards artist usability the more important issues such as fractals not behaving well enough for real world VFX shots can be minimised by the ease in which it is possible to iterate on the fractal design using the renderer.

Despite the final result being a fairly technical tool to use, the artistic feedback from the rest of the team has been vital in getting it to this stage. If I had decided to do this project solo, I don't believe it would have developed beyond the tech demo stage. By having a strong team surrounding me, especially in the compositing department, I saw first hand which features were actually important instead of focusing on the ones that I personally had an interest in. In the end, this is the reason we ended up with 600 frames worth of fractal sequence that was successfully used in integration with a live action piece.
%========================================================

\chapter{Renderer Implementation}

\begin{itemize}
	\item Overview
	\item Sphere tracing distance fields
	\item Path Tracing
	\item Progressive rendering
	\item Tile Based Rendering
	\item Modifying the scene at runtime using Reflection
\end{itemize}

\section{Performance}

\begin{itemize}
	\item Needs to be responsive, at least during the interactive preview
	\item Thought tile based rendering would help
	\item 
	\item rtContextSetTimeoutCallback
\end{itemize}


\section{Scene Management}

\subsection{Saving and loading scenes}
\begin{itemize}
	\item Not usable if scenes can't be saved
	\item Current implementation is more TD oriented, scenes are defined via CUDA functions that are compiled at runtime
	\item Camera information is encoded in comments at the top of the file
\end{itemize}

\subsection{Code Reflection}
Runtime compilation is necessary for allowing scene changes at runtime, specifically the ability to compile to .ptx code so it can be loaded into Optix as a callable function.
\subsubsection{Methods}

\begin{itemize}
	\item NVRTC - Available in CUDA 6.5 and up, faster and built in way of compiling.
	\item System NVCC - If using an older version of CUDA (such as on the university lab workstations), I try to use the system NVCC (hopefully available in path). This has the downsides of requiring the developer tools to be installed, as well as the performance overhead of launching and managing a subprocess. However, this allows me to render on a wider range of machines regardless of the performance impact.
\end{itemize}

\subsection{Runtime Patching}
Once the code is compiled, I needed some way to tell Optix to use it.

\begin{itemize}
	\item Manually patching the generated PTX code ( prone to error )
	\item Optix rtCallablePrograms ( officially supported way of doing it, found out later in the development process )
\end{itemize}

\subsection{Node Graph}

\begin{itemize}
	\item More artist friendly approach
	\item Spent a lot of time trying to develop this in the hope that it would aid the lookdev process
	\item Had to abandon the concept for the sake of getting the fractals lookdev'd on time
	\item Influenced the current design, which is basically a pure code version of what the node graph does
	\item An extra level of indirection, nodes -> CUDA code -> runtime patching
\end{itemize}

\subsubsection{Grammar Definition}
In order to properly write a parser, it was important to treat the node graph like a simple language. This was helpful in figuring out error conditions, for example if a domain operation is plugged into a distance operation \ref{fig:grammar}

\begin{figure}[h!]
	\includegraphics[scale=0.22]{nodegrammar}
	\caption{Grammar Definition}
	\label{fig:grammar}
\end{figure}

%========================================================


\nocite{*}

\bibliographystyle{plain-annote}
\bibliography{bib_icereport}
\end{document}
